{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./skeleton_estimation_train/skeleton_train_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the COCO data set ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'Environment' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-cc76b111b9cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading the COCO data set ...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlmdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m df = MapData(df, lambda x: ( [x[0], x[1], x[2]],\n\u001b[0;32m     10\u001b[0m                               [x[3], x[4], x[3], x[4], x[3], x[4], x[3], x[4], x[3], x[4], x[3], x[4]])\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor2_gpu\\lib\\site-packages\\tensorpack\\dataflow\\common.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, ds, batch_size, remainder, use_list)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mremainder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                 \u001b[1;32massert\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'Environment' has no len()"
     ]
    }
   ],
   "source": [
    "import lmdb\n",
    "from tensorpack.dataflow import *\n",
    "from tensorpack.dataflow.common import BatchData, MapData\n",
    "import numpy as np\n",
    "\n",
    "print (\"Loading the COCO data set ...\")\n",
    "df = lmdb.open(data_path)\n",
    "df = BatchData(df, batch_size, use_list=False)\n",
    "df = MapData(df, lambda x: ( [x[0], x[1], x[2]],\n",
    "                              [x[3], x[4], x[3], x[4], x[3], x[4], x[3], x[4], x[3], x[4], x[3], x[4]])\n",
    "                            )\n",
    "df.reset_state()\n",
    "print(\"Loaded {} training samples!\".format(train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pose_detection_model(n_stage):\n",
    "    # build the input\n",
    "    inputs = []\n",
    "    \n",
    "    img_in = Input( shape = (None, None, 3))\n",
    "    inputs.append(img_in)\n",
    "    w_vector_field_in = Input( shape = (None, None, 38))\n",
    "    inputs.append(w_vector_field_in)\n",
    "    w_confi_in = Input( shape = (None, None, 19))\n",
    "    inputs.append(w_confi_in)\n",
    "\n",
    "    # process the input image\n",
    "    img_processed = Lambda(lambda x: x / 256 - 0.5)(img_in)\n",
    "\n",
    "    #run pure vgg19\n",
    "    vgg_out = vgg_19(img_processed)\n",
    "\n",
    "    # run state 1\n",
    "    stage_1_b1_out = pose_model_stage_1(vgg_out, 38, 1)\n",
    "    w_1_out = Multiply(name = \"weight_stage1_L1\") ( [stage_1_b1_out, w_vector_field_in])\n",
    "\n",
    "    # run state 1\n",
    "    stage_1_b2_out = pose_model_stage_1(vgg_out, 19, 2)\n",
    "    w_2_out = Multiply(name = \"weight_stage1_L2\") ( [stage_1_b2_out, w_confi_in])\n",
    "\n",
    "    x = Concatenate()([stage_1_b1_out, stage_1_b2_out, vgg_out])\n",
    "    # build output\n",
    "    outputs = []\n",
    "    outputs.append(w_1_out)\n",
    "    outputs.append(w_2_out)\n",
    "\n",
    "    for stage_id in range(2,n_stage + 1):\n",
    "        stage_2_b1_out = pose_model_stage_2(x, 38, 1, stage_id)\n",
    "        w_1_out = Multiply(name = \"weight_stage{}_L1\".format(stage_id)) ( [stage_2_b1_out, w_vector_field_in])\n",
    "\n",
    "        stage_2_b2_out = pose_model_stage_2(x, 19, 2, stage_id)\n",
    "        w_2_out = Multiply(name = \"weight_stage{}_L2\".format(stage_id)) ( [stage_2_b2_out, w_confi_in])\n",
    "\n",
    "        outputs.append(w_1_out)\n",
    "        outputs.append(w_2_out)\n",
    "\n",
    "        if (stage_id < n_stage):\n",
    "            x = Concatenate()([stage_2_b1_out, stage_2_b2_out, vgg_out])\n",
    "    # model = Model( inputs = inputs, outputs = outputs)\n",
    "    model = Model(inputs=[img_in], outputs = [stage_2_b1_out, stage_2_b2_out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "def load_keras_vgg_weights(model):\n",
    "    vgg_name = {\n",
    "        'conv1_1': 'block1_conv1',\n",
    "        'conv1_2': 'block1_conv2',\n",
    "        'conv2_1': 'block2_conv1',\n",
    "        'conv2_2': 'block2_conv2',\n",
    "        'conv3_1': 'block3_conv1',\n",
    "        'conv3_2': 'block3_conv2',\n",
    "        'conv3_3': 'block3_conv3',\n",
    "        'conv3_4': 'block3_conv4',\n",
    "        'conv4_1': 'block4_conv1',\n",
    "        'conv4_2': 'block4_conv2'\n",
    "    }\n",
    "    keras_vgg = VGG19(include_top=False, weights='imagenet')\n",
    "    for layer in model.layers:\n",
    "        if layer.name in vgg_name:\n",
    "            layer.set_weights(keras_vgg.get_layer(vgg_name[layer.name]).get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(df):\n",
    "    while True:\n",
    "        for i in df.get_data():\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_funcs():\n",
    "    def _eucl_loss(x, y):\n",
    "        return K.sum(K.square(x - y)) / 10 / 2\n",
    "\n",
    "    losses = {}\n",
    "    losses[\"weight_stage1_L1\"] = _eucl_loss\n",
    "    losses[\"weight_stage1_L2\"] = _eucl_loss\n",
    "    losses[\"weight_stage2_L1\"] = _eucl_loss\n",
    "    losses[\"weight_stage2_L2\"] = _eucl_loss\n",
    "    losses[\"weight_stage3_L1\"] = _eucl_loss\n",
    "    losses[\"weight_stage3_L2\"] = _eucl_loss\n",
    "    losses[\"weight_stage4_L1\"] = _eucl_loss\n",
    "    losses[\"weight_stage4_L2\"] = _eucl_loss\n",
    "    losses[\"weight_stage5_L1\"] = _eucl_loss\n",
    "    losses[\"weight_stage5_L2\"] = _eucl_loss\n",
    "    losses[\"weight_stage6_L1\"] = _eucl_loss\n",
    "    losses[\"weight_stage6_L2\"] = _eucl_loss\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generate_pose_detection_model(6)\n",
    "print(\"Prepare to train a new model\")\n",
    "load_keras_vgg_weights(model)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'sgd', metrics=[\"accuracy\"])\n",
    "model.fit_generator(gen(df), steps_per_epoch=100, epochs=2, use_multiprocessing=False, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
