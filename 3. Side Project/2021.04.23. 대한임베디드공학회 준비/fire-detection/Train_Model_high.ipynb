{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"name":"Train_Model_high.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ybrz67qyy0F9","executionInfo":{"status":"ok","timestamp":1619936793594,"user_tz":-540,"elapsed":1528,"user":{"displayName":"고낙헌","photoUrl":"","userId":"00320345697887287590"}},"outputId":"cd352253-c1cb-4fd0-d4ef-fee2b5a2e558"},"source":["from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"],"id":"ybrz67qyy0F9","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xn3suMKQzwZl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619936793932,"user_tz":-540,"elapsed":1648,"user":{"displayName":"고낙헌","photoUrl":"","userId":"00320345697887287590"}},"outputId":"0e579a1b-7d5e-4692-d888-4f964a63b42b"},"source":["cd /gdrive/MyDrive/fire-detection/"],"id":"xn3suMKQzwZl","execution_count":2,"outputs":[{"output_type":"stream","text":["/gdrive/MyDrive/fire-detection\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"SSudYYFpzwXd","executionInfo":{"status":"ok","timestamp":1619936793934,"user_tz":-540,"elapsed":1500,"user":{"displayName":"고낙헌","photoUrl":"","userId":"00320345697887287590"}},"outputId":"96a61c0c-d4aa-4f35-e65b-c552f643f142"},"source":["pwd"],"id":"SSudYYFpzwXd","execution_count":3,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/gdrive/MyDrive/fire-detection'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"southeast-chapter","executionInfo":{"status":"ok","timestamp":1619936796142,"user_tz":-540,"elapsed":3476,"user":{"displayName":"고낙헌","photoUrl":"","userId":"00320345697887287590"}},"outputId":"414cd378-ef34-46d5-aa3f-9bcdc996de8a"},"source":["import tensorflow as tf\n","import keras_preprocessing\n","from keras_preprocessing import image\n","from keras_preprocessing.image import ImageDataGenerator\n","TRAINING_DIR = \"datasets/Train\"\n","training_datagen = ImageDataGenerator(rescale=1./255,\n","zoom_range=0.15,\n","horizontal_flip=True,\n","fill_mode='nearest')\n","VALIDATION_DIR = \"datasets/Validation\"\n","validation_datagen = ImageDataGenerator(rescale = 1./255)\n","train_generator = training_datagen.flow_from_directory(\n","TRAINING_DIR,\n","target_size=(224,224),\n","shuffle = True,\n","class_mode='categorical',\n","batch_size = 128)\n","validation_generator = validation_datagen.flow_from_directory(\n","VALIDATION_DIR,\n","target_size=(224,224),\n","class_mode='categorical',\n","shuffle = True,\n","batch_size= 14)"],"id":"southeast-chapter","execution_count":4,"outputs":[{"output_type":"stream","text":["Found 2780 images belonging to 2 classes.\n","Found 239 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"surgical-suffering","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619941056482,"user_tz":-540,"elapsed":4263495,"user":{"displayName":"고낙헌","photoUrl":"","userId":"00320345697887287590"}},"outputId":"ccc786ba-f761-4485-da23-89519d34308f"},"source":["from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout\n","\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","es = EarlyStopping(monitor='val_loss', mode='min', patience=5)\n","mc = ModelCheckpoint('CNN_best_model_high.h5', monitor='val_loss', mode='min', save_best_only=True)\n","\n","input_tensor = Input(shape=(224, 224, 3))\n","base_model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(2048, activation='relu')(x)\n","x = Dropout(0.25)(x)\n","x = Dense(1024, activation='relu')(x)\n","x = Dropout(0.2)(x)\n","predictions = Dense(2, activation='softmax')(x)\n","model = Model(inputs=base_model.input, outputs=predictions)\n","for layer in base_model.layers:\n","  layer.trainable = False\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n","history = model.fit(\n","train_generator,\n","steps_per_epoch = 14,\n","epochs = 20,\n","validation_data = validation_generator,\n","validation_steps = 14,\n","callbacks=[es, mc])"],"id":"surgical-suffering","execution_count":5,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","14/14 [==============================] - 502s 35s/step - loss: 19.8648 - acc: 0.6446 - val_loss: 0.1605 - val_acc: 0.9184\n","Epoch 2/20\n","14/14 [==============================] - 263s 18s/step - loss: 0.2586 - acc: 0.9053 - val_loss: 0.2271 - val_acc: 0.9184\n","Epoch 3/20\n","14/14 [==============================] - 247s 18s/step - loss: 0.1564 - acc: 0.9480 - val_loss: 0.3678 - val_acc: 0.8163\n","Epoch 4/20\n","14/14 [==============================] - 252s 18s/step - loss: 0.1527 - acc: 0.9319 - val_loss: 0.1236 - val_acc: 0.9541\n","Epoch 5/20\n","14/14 [==============================] - 245s 17s/step - loss: 0.1901 - acc: 0.9311 - val_loss: 0.1355 - val_acc: 0.9439\n","Epoch 6/20\n","14/14 [==============================] - 250s 18s/step - loss: 0.4619 - acc: 0.9002 - val_loss: 0.0944 - val_acc: 0.9592\n","Epoch 7/20\n","14/14 [==============================] - 244s 17s/step - loss: 0.1686 - acc: 0.9367 - val_loss: 0.1653 - val_acc: 0.9337\n","Epoch 8/20\n","14/14 [==============================] - 248s 18s/step - loss: 0.1913 - acc: 0.9233 - val_loss: 0.6346 - val_acc: 0.8061\n","Epoch 9/20\n","14/14 [==============================] - 251s 18s/step - loss: 0.2174 - acc: 0.9284 - val_loss: 0.9049 - val_acc: 0.7500\n","Epoch 10/20\n","14/14 [==============================] - 245s 17s/step - loss: 0.5001 - acc: 0.8787 - val_loss: 0.1606 - val_acc: 0.9388\n","Epoch 11/20\n","14/14 [==============================] - 251s 18s/step - loss: 0.0707 - acc: 0.9800 - val_loss: 0.0930 - val_acc: 0.9541\n","Epoch 12/20\n","14/14 [==============================] - 245s 17s/step - loss: 0.0953 - acc: 0.9618 - val_loss: 0.1944 - val_acc: 0.9439\n","Epoch 13/20\n","14/14 [==============================] - 243s 17s/step - loss: 0.1933 - acc: 0.9267 - val_loss: 0.0976 - val_acc: 0.9643\n","Epoch 14/20\n","14/14 [==============================] - 252s 18s/step - loss: 0.1114 - acc: 0.9618 - val_loss: 0.1365 - val_acc: 0.9490\n","Epoch 15/20\n","14/14 [==============================] - 245s 17s/step - loss: 0.1106 - acc: 0.9722 - val_loss: 0.1115 - val_acc: 0.9490\n","Epoch 16/20\n","14/14 [==============================] - 242s 17s/step - loss: 0.0680 - acc: 0.9801 - val_loss: 0.5540 - val_acc: 0.8673\n"],"name":"stdout"}]}]}